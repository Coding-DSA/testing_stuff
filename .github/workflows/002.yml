on:
  push:
  workflow_dispatch:

jobs:
  run-script:
    runs-on: ubuntu-latest
    environment: dev
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Verify Files After Checkout
        run: ls -R  # Debugging: Ensure PythonFolder and CSV exist

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests azure-identity azure-mgmt-purview azure-mgmt-resource azure-purview-catalog

      - name: Create Sample CSV File (For Testing, If Missing)
        run: |
          if [ ! -f "PythonFolder/azure_cosmos_db_onboarding.csv" ]; then
            mkdir -p PythonFolder
            echo "id,DatasourceName,DatabaseName,Kind,Endpoint" > PythonFolder/azure_cosmos_db_onboarding.csv
            echo "1,TestSource,testdb,AzureCosmosDB,https://example.com" >> PythonFolder/azure_cosmos_db_onboarding.csv
          fi

      - name: Read Parameter File and Onboard New Data Sources
        id: read_csv_python
        run: |
          python - <<EOF
          import csv, os, json

          file_path = "PythonFolder/azure_cosmos_db_onboarding.csv"
          if not os.path.exists(file_path):
              raise FileNotFoundError(f"File not found: {file_path}")

          with open(file_path, "r") as file:
              reader = csv.DictReader(file)
              data_rows = list(reader)

          # Simulating filtering of new data sources
          filtered_data = [row for row in data_rows if row["DatabaseName"] not in ["ExistingDB1", "ExistingDB2"]]

          # Save filtered data to a JSON file
          with open("filtered_data.json", "w") as f:
              json.dump(filtered_data, f, indent=4)

          print("Filtered Data Saved to filtered_data.json")
          EOF

      - name: Upload Filtered Data as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: filtered-data
          path: filtered_data.json

  processed-data:
    needs: run-script
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download Filtered Data Artifact
        uses: actions/download-artifact@v4
        with:
          name: filtered-data

      - name: Process Filtered Data
        run: |
          echo "Processing task data..."
          
          if [ ! -f "filtered_data.json" ]; then
            echo "Error: Filtered data file not found!"
            exit 1
          fi

          cat filtered_data.json | python - <<EOF
          import sys, json

          try:
              decoded_json = json.load(sys.stdin)
          except Exception as e:
              print(f"JSON decoding error: {e}")
              sys.exit(1)

          if not decoded_json:
              print("No new data sources found.")
              sys.exit(0)

          for item in decoded_json:
              print("Deploying data source:")
              for key, value in item.items():
                  print(f"- {key}: {value}")
          EOF
