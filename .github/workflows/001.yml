name: Test Read Row from CSV

on:
  push:
  workflow_dispatch:

jobs:
  run-script:
    runs-on: ubuntu-latest
    outputs:
      filtered_data: ${{ steps.process_csv.outputs.filtered_data }} 
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Create dummy CSV
        run: |
          mkdir -p PythonFolder
          echo "DatabaseName,Environment,Description" > PythonFolder/azure_cosmos_db_onboarding.csv
          echo "TestDB1,dev,First test database" >> PythonFolder/azure_cosmos_db_onboarding.csv
          echo "TestDB2,prod,Second test database" >> PythonFolder/azure_cosmos_db_onboarding.csv
          echo "TestDB3,test,Third test database" >> PythonFolder/azure_cosmos_db_onboarding.csv

      - name: Process CSV and Filter Data
        id: process_csv
        run: |
          # Create an array of existing databases
          existing_databases=("TestDB2")

          # Use awk to filter the CSV data
          filtered_data=$(awk -F',' 'NR>1 && !($1 in existing_databases) {print $0}' PythonFolder/azure_cosmos_db_onboarding.csv)

          # Save filtered_data as an environment variable
          echo "filtered_data=$filtered_data" >> $GITHUB_ENV

        outputs:
          filtered_data: ${{ env.filtered_data }}

  processed-data:
    needs: run-script
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Process filtered data
        run: |
          echo "Processing task data..."
          task_data="${{ needs.run-script.outputs.filtered_data }}" 

          if [[ -n "$task_data" ]]; then
            while IFS=',' read -r db_name environment description; do
              echo "Simulated deployment of data source:"
              echo "- DatabaseName: $db_name"
              echo "- Environment: $environment"
              echo "- Description: $description"
            done <<< "$task_data"
          else
            echo "No tasks to process."
          fi
